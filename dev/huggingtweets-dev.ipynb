{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXeYEzBG7g0H"
      },
      "source": [
        "# HuggingTweets - Tweet Generation with Huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZuE848q7g0I"
      },
      "source": [
        "*Disclaimer: this project is not to be used to publish any false generated information but to perform research on Natural Language Generation (NLG).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "668-beiK7g0J"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbjY0dPNyVAO"
      },
      "source": [
        "## Pre colab\n",
        "\n",
        "\n",
        "\n",
        "1. Download csvfilter.py\n",
        "2. Download the discord messages for the person as csv\n",
        "3. merge the csv's together\n",
        "4. run csvfilter.py and follow instructions\n",
        "5. once it asks for content and number removal go to edit-csv and remove the first columm and row\n",
        "6. when it asks for whitename removal execute the powershell script then press return on the script\n",
        "7. the ouput will be the fully filtered csv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wODF0Bx27g0K"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxRDBrFi7g0L",
        "outputId": "9e65bae8-0405-4eae-f1bd-d10fcab54cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3 MB 5.0 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 895 kB 47.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9 MB 42.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2 MB 39.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8 MB 5.1 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 61.4 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144 kB 61.9 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 2.2 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# install required libraries are not installed\n",
        "!pip install torch -qq\n",
        "!pip install transformers==v3.4.0 -qq\n",
        "!pip install wandb==v0.12.15 -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IRQjZw37g0P",
        "outputId": "af7d3540-4d5b-47a7-d877-f1cc9f53f61d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-23 20:22:44--  https://raw.githubusercontent.com/huggingface/transformers/v3.3.0/examples/language-modeling/run_language_modeling.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11434 (11K) [text/plain]\n",
            "Saving to: â€˜run_language_modeling.pyâ€™\n",
            "\n",
            "\rrun_language_modeli   0%[                    ]       0  --.-KB/s               \rrun_language_modeli 100%[===================>]  11.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-23 20:22:44 (97.6 MB/s) - â€˜run_language_modeling.pyâ€™ saved [11434/11434]\n",
            "\n",
            "--2022-04-23 20:22:44--  https://raw.githubusercontent.com/huggingface/transformers/v3.5.1/examples/text-generation/run_generation.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11046 (11K) [text/plain]\n",
            "Saving to: â€˜run_generation.pyâ€™\n",
            "\n",
            "run_generation.py   100%[===================>]  10.79K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-23 20:22:45 (87.3 MB/s) - â€˜run_generation.pyâ€™ saved [11046/11046]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# HuggingFace scripts for fine-tuning models and language generation\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/v3.3.0/examples/language-modeling/run_language_modeling.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/v3.5.1/examples/text-generation/run_generation.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9IU6X-EyVAa"
      },
      "source": [
        "## Filter\n",
        "\n",
        "filter the csv file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aJIFGgG4yVAd"
      },
      "outputs": [],
      "source": [
        "from array import array\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "P0QKRAtpyVAi",
        "outputId": "b3e60e91-01ee-4d65-88f5-afb9ad3261a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use Py scripts to generate dataset csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aef12f15-d2ef-46a7-8ad2-ebae6ddfd5ef\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aef12f15-d2ef-46a7-8ad2-ebae6ddfd5ef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving cleanNVG.csv to cleanNVG.csv\n",
            "cleanNVG.csv  run_generation.py  run_language_modeling.py  sample_data\n",
            "enter csv namecleanNVG.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "print(\"Use Py scripts to generate dataset csv\")\n",
        "files.upload()\n",
        "!ls\n",
        "csvanme = input(\"enter csv name\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9grEfrxIyVAk"
      },
      "source": [
        "## Make Dataset\n",
        "\n",
        "help me please\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dEEvUqD9yVAo"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "import torch\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QzNneKQ6yVAp"
      },
      "outputs": [],
      "source": [
        "handle = 'JustinHughes'\n",
        "with open(csvanme, newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
        "    reader = list(reader)\n",
        "# shuffle data\n",
        "    random.shuffle(reader)\n",
        "\n",
        "# fraction of training data\n",
        "    split_train_valid = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "__7AThcQyVAq"
      },
      "outputs": [],
      "source": [
        "# split dataset\n",
        "with open(csvanme, newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
        "    reader = list(reader)\n",
        "train_size = int(split_train_valid * len(reader))\n",
        "valid_size = len(reader) - train_size\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(reader, [train_size, valid_size]) \n",
        "\n",
        "def make_dataset(dataset, epochs):\n",
        "\n",
        "  total_text = '<|endoftext|>'\n",
        "  reader = [t for t in dataset]\n",
        "  for _ in range(epochs):\n",
        "\n",
        "    random.shuffle(reader)\n",
        "    total_text += '<|endoftext|>'.join(map(str, reader)) + '<|endoftext|>'\n",
        "  return total_text\n",
        "EPOCHS = 4\n",
        "\n",
        "with open('{}_train.txt'.format(handle), 'w') as f:\n",
        "\n",
        "  data = make_dataset(train_dataset, EPOCHS)\n",
        "  f.write(data)\n",
        "\n",
        "with open('{}_valid.txt'.format(handle), 'w') as f:\n",
        "  data = make_dataset(valid_dataset, 1)\n",
        "  f.write(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKfJrfl57g1J"
      },
      "source": [
        "## Log and monitor training through W&B\n",
        "\n",
        "In order to check our model is training correctly and compare experiments, we are going to use the W&B integration from HuggingFace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD3v1dDK7g1J"
      },
      "source": [
        "### API Key\n",
        "Once you've signed up, run the next cell and click on the link to get your API key and authenticate this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDu7g4dy7g1K",
        "outputId": "91519c26-8712-40b5-9f4e-c906d21815d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "!wandb login --relogin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeCgMAKo7g1N"
      },
      "source": [
        "## Fine-tuning the model\n",
        "\n",
        "HuggingFace includes the script `run_language_modeling` making it easy to fine-tune a pre-trained model.\n",
        "\n",
        "We use a pre-trained GPT-2 model and fine-tune it on our dataset.\n",
        "\n",
        "Training is automatically logged on W&B (see [documentation](https://docs.wandb.com/huggingface)). Urls are generated to visualize ongoing runs or you can just open your [dashboard](http://app.wandb.ai/).\n",
        "\n",
        "IÂ quickly tested running for several epochs and my run was showing I started overfitting after 4 epochs so this is the limit I use to fine-tune my model (takes less than 2 minutes).\n",
        "\n",
        "![](https://i.imgur.com/1uIxLFe.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA0SYlIU7g1N",
        "outputId": "e227e332-0645-45aa-f199-aba017f9001c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: WANDB_PROJECT=huggingtweets-dev\n"
          ]
        }
      ],
      "source": [
        "# Associate run to a project (optional)\n",
        "%env WANDB_PROJECT=huggingtweets-dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJQoCJV97g1Q"
      },
      "source": [
        "We use HuggingFace script `run_language_modeling.py` to fine-tune our model (see [doc](https://huggingface.co/transformers/)).\n",
        "\n",
        "*Note: epochs are built into the dataset*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4LWV56z7g1Q",
        "outputId": "d19ec189-a6cf-41d9-e8ea-38a2b6bb883c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04/23/2022 20:24:16 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/23/2022 20:24:16 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output/JustinHughes', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=1, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Apr23_20-24-16_34b79911ee29', logging_first_step=False, logging_steps=20, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=20, dataloader_num_workers=0, past_index=-1, run_name='output/JustinHughes', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "Downloading: 100% 665/665 [00:00<00:00, 672kB/s]\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 6.18MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 8.51MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_auto.py:825: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Downloading: 100% 548M/548M [00:09<00:00, 57.4MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1374: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:263: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead. Setting `args.prediction_loss_only=True\n",
            "  FutureWarning,\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mampheco\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220423_202445-35ig97nn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33moutput/JustinHughes\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ampheco/huggingtweets-dev\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ampheco/huggingtweets-dev/runs/35ig97nn\u001b[0m\n",
            " 15% 20/132 [00:07<00:40,  2.74it/s]{'loss': 2.764982986450195, 'learning_rate': 4.242424242424243e-05, 'epoch': 0.15151515151515152, '_timestamp': 1650745496, '_runtime': 11}\n",
            "{'loss': 2.3306095123291017, 'learning_rate': 3.484848484848485e-05, 'epoch': 0.30303030303030304, '_timestamp': 1650745503, '_runtime': 18}\n",
            "{'loss': 2.200123596191406, 'learning_rate': 2.7272727272727273e-05, 'epoch': 0.45454545454545453, '_timestamp': 1650745511, '_runtime': 26}\n",
            "{'loss': 2.081360626220703, 'learning_rate': 1.9696969696969697e-05, 'epoch': 0.6060606060606061, '_timestamp': 1650745518, '_runtime': 33}\n",
            "{'loss': 2.0674095153808594, 'learning_rate': 1.2121212121212122e-05, 'epoch': 0.7575757575757576, '_timestamp': 1650745526, '_runtime': 41}\n",
            " 91% 120/132 [00:44<00:04,  2.64it/s]{'loss': 1.9997413635253907, 'learning_rate': 4.5454545454545455e-06, 'epoch': 0.9090909090909091, '_timestamp': 1650745533, '_runtime': 48}\n",
            "100% 132/132 [00:49<00:00,  2.67it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
            "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
            "04/23/2022 20:25:40 - INFO - __main__ -   *** Evaluate ***\n",
            "  0% 0/1 [00:00<?, ?it/s]04/23/2022 20:25:40 - INFO - __main__ -   ***** Eval results *****\n",
            "04/23/2022 20:25:40 - INFO - __main__ -     perplexity = 7.835818440515906\n",
            "100% 1/1 [00:00<00:00,  3.24it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch â–â–‚â–„â–…â–†â–‡â–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval_loss â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate â–ˆâ–‡â–…â–„â–‚â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          loss â–ˆâ–„â–ƒâ–‚â–‚â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    total_flos â–â–‚â–ƒâ–…â–†â–‡â–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval_loss 2.05871\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          loss 1.99974\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    total_flos 100921679806464\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33moutput/JustinHughes\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ampheco/huggingtweets-dev/runs/35ig97nn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220423_202445-35ig97nn/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --output_dir=output/$handle \\\n",
        "    --overwrite_output_dir \\\n",
        "    --overwrite_cache \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2 \\\n",
        "    --do_train --train_data_file=$handle\\_train.txt \\\n",
        "    --do_eval --eval_data_file=$handle\\_valid.txt \\\n",
        "    --eval_steps 20 \\\n",
        "    --logging_steps 20 \\\n",
        "    --per_gpu_train_batch_size 1 \\\n",
        "    --num_train_epochs 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viSAJ7EE7g1T"
      },
      "source": [
        "## Let's test our trained model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffHmhI9P7g1T"
      },
      "source": [
        "We test our model on a few sample sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uQRNMedK7g1T"
      },
      "outputs": [],
      "source": [
        "SENTENCES = [\"I think that\",\n",
        "             \"I like\",\n",
        "             \"I don't like\",\n",
        "             \"I want\",\n",
        "             \"My dream is\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImkNghlH7g1V"
      },
      "source": [
        "We use HuggingFace script `run_generation.py` to generate sentences (see [doc](https://huggingface.co/transformers/))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK-R3bPP7g1W",
        "outputId": "c85b119a-063f-413b-c72d-725db9ae9a24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1086660266"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import random\n",
        "seed = random.randint(0, 2**32-1)\n",
        "seed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "qc6a9yL64mmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkD5CRkn7g1Y",
        "outputId": "aa12b18d-fd38-4a38-c902-9f0185963193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of sentence: I think that\n",
            "* Generated #1: === GENERATED SEQUENCE 3 ===\n",
            "* Generated #2: \n",
            "* Generated #3: Just in case everyone else thought I might've been a little misguided, thanks!\n",
            "* Generated #4: I think that's cool, but actually until someone takes the plunge I'm not sure how long they're gonna wait, which i'm guessing is until you're shit.\" -@ryginthenothing\n",
            "* Generated #5: I think that the commander-in-chief needs to find a better way to recruit the soldiers, particularly in a modern-day Iraq where sectarian tensions between Sunni tribes are even wider, and if the rules of engagement are not enforced, they'll emerge as a threat to the brand-new constitution.\n",
            "\n",
            "Start of sentence: I like\n",
            "* Generated #1: I like fun! It's just until someone gets frustrated, laughs, makes fun of you...eyersomieyeU! If you've gotten much shitim laughing, come on! I'm super sorry.\"\n",
            "* Generated #2: \"You're right, too.\"\n",
            "* Generated #3: \"You're\"\n",
            "* Generated #4: I like the portraits! And the majority of men, haha.\n",
            "* Generated #5: ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ï¿½\n",
            "\n",
            "Start of sentence: I don't like\n",
            "* Generated #1: Thankyou, Sam.\n",
            "* Generated #2: I don't like fun\" It's actually much funnier than: \"How's it going?\"\n",
            "* Generated #3: I don't like any of this, but that's what's good.\n",
            "* Generated #4: ðŸ˜±\n",
            "* Generated #5: Elijah Fipart\n",
            "\n",
            "Start of sentence: I want\n",
            "* Generated #1: --------------------\n",
            "* Generated #2: PLEASE COMMENT!' |Back door', 'become', 'a','movie', 'lover', 'How', 'gorgeous', 'everyone', 'looks', 'now!'}\n",
            "* Generated #3: I want fun! It's just until someone gets hurt, laughs, makes fun of her?!? â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ï¿½\n",
            "* Generated #4: I want to explore what's motivating people to \"finish out\" their trading:\n",
            "* Generated #5: Elijah Fipart\n",
            "\n",
            "Start of sentence: My dream is\n",
            "* Generated #1: My dream is to be able to sign a contract with @rolesdraggolf\",\"thp\" }, { \"hacked\": \"2072\", \"how\": \"2016-10-17\", \"thing\" }, { \"dokoodlick\": \"!!\" }], \"BROWN: YOO\" }, { \"reviews\": {\"are-one\", \"new\", \"votes\", \"1120\", \"4:3\", \"\", \"unexpected\", \"rules:\", \"shouldnt\"have\", \"been\", \"time-for\", \"events!\", \" https://twitter.com/aaronovl/status/10053750502500013330? \"but-leave-u\", \"and\", \"give\", \"me\", \"20\n",
            "* Generated #2: My dream is my dog! ^^)\n",
            "* Generated #3: My dream is if i'm making every frame of iPhone's without problems, etc, you'll all be there! <3\n",
            "* Generated #4: My dream is always to be in esports and get some love, laughs, opinions and ideas (they're my dream, which i'm admiring)\n",
            "* Generated #5: My dream is to lead MIT, a master-planned, creative, free, Linux-based, decentralized data center, with an enviroment surrounding itself. Also, I'd love if your friends and colleagues could help.\" â€• Jonathan\n"
          ]
        }
      ],
      "source": [
        "examples = []\n",
        "num_return_sequences = 5\n",
        "\n",
        "for start in SENTENCES:\n",
        "    val = !python run_generation.py \\\n",
        "        --model_type gpt2 \\\n",
        "        --model_name_or_path output/$handle \\\n",
        "        --length 160 \\\n",
        "        --num_return_sequences $num_return_sequences \\\n",
        "        --temperature 1 \\\n",
        "        --p 0.95 \\\n",
        "        --seed $seed \\\n",
        "        --prompt {'\"<|endoftext|>' + start + '\"'}\n",
        "    generated = [val[-1-2*k] for k in range(num_return_sequences)[::-1]]\n",
        "    print(f'\\nStart of sentence: {start}')\n",
        "    for i, g in enumerate(generated):\n",
        "        g = g.replace('<|endoftext|>', '')\n",
        "        print(f'* Generated #{i+1}: {g}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6kHI7_D7g1g"
      },
      "source": [
        "## About\n",
        "\n",
        "*Built by Boris Dayma* *Modified by UN! and Amphe Co.*\n",
        "\n",
        "[![Follow](https://img.shields.io/twitter/follow/borisdayma?style=social)](https://twitter.com/intent/follow?screen_name=borisdayma)\n",
        "\n",
        "[![Follow](https://img.shields.io/twitter/follow/lilunsimp?style=social)](https://twitter.com/intent/follow?screen_name=lilunsimp)\n",
        "\n",
        "[![Follow](https://img.shields.io/twitter/follow/ampheos?style=social)](https://twitter.com/intent/follow?screen_name=ampheos)\n",
        "\n",
        "[![GitHub followers](https://img.shields.io/github/followers/un-simp?label=un-simp&style=social)](https://github.com/un-simp)\n",
        "\n",
        "[![GitHub Org's stars](https://img.shields.io/github/stars/amphe-os?label=amphe-OS&style=social)](https://github.com/amphe-os)\n",
        "\n",
        "My main goals with this project are:\n",
        "* to experiment with how to train, deploy and maintain neural networks in production ;\n",
        "* to make AI accessible to everyone ;\n",
        "* to have fun!\n",
        "\n",
        "For more details, visit the project repository.\n",
        "\n",
        "[![GitHub stars](https://img.shields.io/github/stars/borisdayma/huggingtweets?style=social)](https://github.com/borisdayma/huggingtweets)\n",
        "\n",
        "**Disclaimer: this project is not to be used to publish any false generated information but to perform research on Natural Language Generation.**\n",
        "\n",
        "## Resources\n",
        "\n",
        "* [Explore the W&B report](https://app.wandb.ai/wandb/huggingtweets/reports/HuggingTweets-Train-a-model-to-generate-tweets--VmlldzoxMTY5MjI) to understand how the model works\n",
        "* [HuggingFace and W&B integration documentation](https://docs.wandb.com/library/integrations/huggingface)\n",
        "\n",
        "## Got questions about W&B?\n",
        "\n",
        "If you have any questions about using W&B to track your model performance and predictions, please reach out to the [slack community](http://bit.ly/wandb-forum)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "huggingtweets-dev.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}